{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "\n",
      "# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/exceptions.py\", line 1079, in __call__\n",
      "        return func(*args, **kwargs)\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/cli/main.py\", line 84, in _main\n",
      "        exit_code = do_call(args, p)\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/cli/conda_argparse.py\", line 83, in do_call\n",
      "        return getattr(module, func_name)(args, parser)\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/cli/main_install.py\", line 20, in execute\n",
      "        install(args, parser, 'install')\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/cli/install.py\", line 116, in install\n",
      "        if context.use_only_tar_bz2:\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda/base/context.py\", line 714, in use_only_tar_bz2\n",
      "        import conda_package_handling.api\n",
      "      File \"/opt/anaconda3/lib/python3.7/site-packages/conda_package_handling/api.py\", line 140, in <module>\n",
      "        tmpdir_root=_tempfile.gettempdir(), processes=None):\n",
      "      File \"/opt/anaconda3/lib/python3.7/tempfile.py\", line 294, in gettempdir\n",
      "        tempdir = _get_default_tempdir()\n",
      "      File \"/opt/anaconda3/lib/python3.7/tempfile.py\", line 229, in _get_default_tempdir\n",
      "        dirlist)\n",
      "    FileNotFoundError: [Errno 2] No usable temporary directory found in ['/var/folders/cj/1hmcms253gzgxrxwr75f8yy40000gn/T/', '/tmp', '/var/tmp', '/usr/tmp', '/Users/Pablo/Documents/GitHub/sentiment-analysis-project/RNN']\n",
      "\n",
      "`$ /opt/anaconda3/bin/conda install --yes --prefix /opt/anaconda3 -c conda-forge spacy`\n",
      "\n",
      "  environment variables:\n",
      "                 CIO_TEST=<not set>\n",
      "        CONDA_DEFAULT_ENV=base\n",
      "                CONDA_EXE=/opt/anaconda3/bin/conda\n",
      "             CONDA_PREFIX=/opt/anaconda3\n",
      "    CONDA_PROMPT_MODIFIER=(base)\n",
      "         CONDA_PYTHON_EXE=/opt/anaconda3/bin/python\n",
      "               CONDA_ROOT=/opt/anaconda3\n",
      "              CONDA_SHLVL=1\n",
      "           CURL_CA_BUNDLE=<not set>\n",
      "                     PATH=/opt/anaconda3/bin:/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bi\n",
      "                          n:/usr/sbin:/sbin:/usr/local/share/dotnet:/opt/X11/bin:/Library/Apple/\n",
      "                          usr/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/\n",
      "                          Applications/Xamarin Workbooks.app/Contents/SharedSupport/path-bin\n",
      "         PYTHONIOENCODING=utf-8\n",
      "               PYTHONPATH=/Users/Pablo/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/p\n",
      "                          ythonFiles:/Users/Pablo/.vscode/extensions/ms-\n",
      "                          toolsai.jupyter-2020.12.414227025/pythonFiles/lib/python\n",
      "         PYTHONUNBUFFERED=1\n",
      "           PYTHONWARNINGS=ignore\n",
      "       REQUESTS_CA_BUNDLE=<not set>\n",
      "            SSL_CERT_FILE=<not set>\n",
      "\n",
      "     active environment : base\n",
      "    active env location : /opt/anaconda3\n",
      "            shell level : 1\n",
      "       user config file : /Users/Pablo/.condarc\n",
      " populated config files : /Users/Pablo/.condarc\n",
      "          conda version : 4.9.2\n",
      "    conda-build version : 3.18.9\n",
      "         python version : 3.7.4.final.0\n",
      "       virtual packages : __osx=10.15.5=0\n",
      "                          __unix=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : /opt/anaconda3  (writable)\n",
      "           channel URLs : https://conda.anaconda.org/conda-forge/osx-64\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\n",
      "                          https://repo.anaconda.com/pkgs/main/osx-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/osx-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /opt/anaconda3/pkgs\n",
      "                          /Users/Pablo/.conda/pkgs\n",
      "       envs directories : /opt/anaconda3/envs\n",
      "                          /Users/Pablo/.conda/envs\n",
      "               platform : osx-64\n",
      "             user-agent : conda/4.9.2 requests/2.22.0 CPython/3.7.4 Darwin/19.5.0 OSX/10.15.5\n",
      "                UID:GID : 501:20\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "An unexpected error has occurred. Conda has prepared the above report.\n",
      "\n",
      "Upload successful.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Stuff\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, LabelField\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# spacy_en = spacy.load('en')\n",
    "\n",
    "#     return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "title = Field(sequential=True, use_vocab=True, tokenize='spacy', lower=True)\n",
    "text = Field(sequential=True, use_vocab=True, tokenize='spacy', include_lengths = True, lower=True)\n",
    "label = LabelField(dtype=torch.float)\n",
    "\n",
    "fields = {'text': ('text', text), 'label': ('label', label), 'title': ('title', title)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num of training:  6335\n"
     ]
    }
   ],
   "source": [
    "train_data = TabularDataset.splits(\n",
    "    path='../data',\n",
    "    train='news.csv',\n",
    "    # validation='news.csv',\n",
    "    # test='news.csv',\n",
    "    format='csv',\n",
    "    fields=fields)[0]\n",
    "print(\"Num of training: \", len(train_data))\n",
    "# print(\"Num of validation: \", len(validation_data))\n",
    "# print(\"Num of testing: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num of training:  4118\nNum of validation:  1108\nNum of testing:  1109\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_data.split(split_ratio=0.65)\n",
    "validation_data, test_data = validation_data.split(split_ratio=0.5)\n",
    "print(\"Num of training: \", len(train_data))\n",
    "print(\"Num of validation: \", len(validation_data))\n",
    "print(\"Num of testing: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': ['hillary',\n",
       "  'clinton',\n",
       "  '’s',\n",
       "  'presidential',\n",
       "  'campaign',\n",
       "  ',',\n",
       "  'her',\n",
       "  'network',\n",
       "  'of',\n",
       "  'super',\n",
       "  'political',\n",
       "  'action',\n",
       "  'committees',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'liberal',\n",
       "  'establishment',\n",
       "  'relished',\n",
       "  'a',\n",
       "  'matchup',\n",
       "  'against',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  '.',\n",
       "  'however',\n",
       "  ',',\n",
       "  'her',\n",
       "  'campaign',\n",
       "  'failed',\n",
       "  'to',\n",
       "  'put',\n",
       "  'forward',\n",
       "  'an',\n",
       "  'alternative',\n",
       "  'for',\n",
       "  'voters',\n",
       "  'that',\n",
       "  'would',\n",
       "  'combat',\n",
       "  'a',\n",
       "  'candidate',\n",
       "  'that',\n",
       "  'tapped',\n",
       "  'into',\n",
       "  'the',\n",
       "  'vast',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'disillusionment',\n",
       "  'among',\n",
       "  'citizens',\n",
       "  '.',\n",
       "  'tsunamis',\n",
       "  'of',\n",
       "  'voters',\n",
       "  'unaccounted',\n",
       "  'for',\n",
       "  'in',\n",
       "  'state',\n",
       "  'polls',\n",
       "  ',',\n",
       "  'who',\n",
       "  'do',\n",
       "  'not',\n",
       "  'identify',\n",
       "  'with',\n",
       "  'either',\n",
       "  'the',\n",
       "  'democratic',\n",
       "  'or',\n",
       "  'republican',\n",
       "  'parties',\n",
       "  ',',\n",
       "  'made',\n",
       "  'president',\n",
       "  'trump',\n",
       "  'a',\n",
       "  'reality',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'clinton',\n",
       "  '’s',\n",
       "  'concession',\n",
       "  'speech',\n",
       "  'indicated',\n",
       "  'the',\n",
       "  'campaign',\n",
       "  'and',\n",
       "  'many',\n",
       "  'of',\n",
       "  'its',\n",
       "  'supporters',\n",
       "  'are',\n",
       "  'unwilling',\n",
       "  'to',\n",
       "  'confront',\n",
       "  'the',\n",
       "  'hubris',\n",
       "  'of',\n",
       "  'her',\n",
       "  'presidential',\n",
       "  'run',\n",
       "  '.',\n",
       "  'yet',\n",
       "  ',',\n",
       "  'citizens',\n",
       "  ',',\n",
       "  'especially',\n",
       "  'those',\n",
       "  'on',\n",
       "  'the',\n",
       "  'left',\n",
       "  ',',\n",
       "  'must',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'find',\n",
       "  'the',\n",
       "  'clarity',\n",
       "  'to',\n",
       "  'move',\n",
       "  'onward',\n",
       "  'with',\n",
       "  'fights',\n",
       "  'for',\n",
       "  'social',\n",
       "  ',',\n",
       "  'economic',\n",
       "  ',',\n",
       "  'racial',\n",
       "  ',',\n",
       "  'and',\n",
       "  'environmental',\n",
       "  'justice',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'the',\n",
       "  'democratic',\n",
       "  'party',\n",
       "  'rigged',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'party',\n",
       "  '’s',\n",
       "  'primary',\n",
       "  'for',\n",
       "  'clinton',\n",
       "  ',',\n",
       "  'and',\n",
       "  'it',\n",
       "  'helped',\n",
       "  'stave',\n",
       "  'off',\n",
       "  'a',\n",
       "  'decisive',\n",
       "  'challenge',\n",
       "  'from',\n",
       "  'senator',\n",
       "  'bernie',\n",
       "  'sanders',\n",
       "  '.',\n",
       "  'the',\n",
       "  'senator',\n",
       "  'addressed',\n",
       "  'the',\n",
       "  'material',\n",
       "  'conditions',\n",
       "  'of',\n",
       "  'the',\n",
       "  'working',\n",
       "  'class',\n",
       "  ',',\n",
       "  'including',\n",
       "  'people',\n",
       "  'of',\n",
       "  'color',\n",
       "  '.',\n",
       "  'he',\n",
       "  'warned',\n",
       "  'the',\n",
       "  'democrats',\n",
       "  'of',\n",
       "  'wealth',\n",
       "  'inequality',\n",
       "  ',',\n",
       "  'destructive',\n",
       "  'free',\n",
       "  'trade',\n",
       "  'agreements',\n",
       "  ',',\n",
       "  'and',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'negative',\n",
       "  'effects',\n",
       "  'of',\n",
       "  'global',\n",
       "  'capitalism',\n",
       "  'on',\n",
       "  'the',\n",
       "  'common',\n",
       "  'man',\n",
       "  'or',\n",
       "  'woman',\n",
       "  '.',\n",
       "  'he',\n",
       "  'connected',\n",
       "  'with',\n",
       "  'disaffected',\n",
       "  'people',\n",
       "  'who',\n",
       "  'the',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'effectively',\n",
       "  'wrote',\n",
       "  '-',\n",
       "  'off',\n",
       "  'and',\n",
       "  'performed',\n",
       "  'well',\n",
       "  'in',\n",
       "  'states',\n",
       "  'that',\n",
       "  'clinton',\n",
       "  'lost',\n",
       "  'in',\n",
       "  'the',\n",
       "  'general',\n",
       "  'election',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'however',\n",
       "  ',',\n",
       "  'the',\n",
       "  'democratic',\n",
       "  'party',\n",
       "  'elites',\n",
       "  'survived',\n",
       "  'and',\n",
       "  'coerced',\n",
       "  'sanders',\n",
       "  'and',\n",
       "  'his',\n",
       "  'supporters',\n",
       "  'into',\n",
       "  'falling',\n",
       "  'in',\n",
       "  'line',\n",
       "  'at',\n",
       "  'their',\n",
       "  'national',\n",
       "  'convention',\n",
       "  '.',\n",
       "  'the',\n",
       "  'party',\n",
       "  'leadership',\n",
       "  'enforced',\n",
       "  'unity',\n",
       "  'in',\n",
       "  'philadelphia',\n",
       "  'to',\n",
       "  'make',\n",
       "  'it',\n",
       "  'appear',\n",
       "  'as',\n",
       "  'if',\n",
       "  'all',\n",
       "  'was',\n",
       "  'well',\n",
       "  'when',\n",
       "  'that',\n",
       "  'was',\n",
       "  'not',\n",
       "  'the',\n",
       "  'case',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'most',\n",
       "  'progressive',\n",
       "  'groups',\n",
       "  ',',\n",
       "  'like',\n",
       "  'all',\n",
       "  'presidential',\n",
       "  'elections',\n",
       "  ',',\n",
       "  'demobilized',\n",
       "  'or',\n",
       "  'essentially',\n",
       "  'became',\n",
       "  'mechanisms',\n",
       "  'for',\n",
       "  'the',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'to',\n",
       "  'mobilize',\n",
       "  'voters',\n",
       "  'from',\n",
       "  'august',\n",
       "  'to',\n",
       "  'election',\n",
       "  'day',\n",
       "  '.',\n",
       "  'this',\n",
       "  'allowed',\n",
       "  'the',\n",
       "  'message',\n",
       "  'of',\n",
       "  '“',\n",
       "  'never',\n",
       "  'trump',\n",
       "  '”',\n",
       "  'to',\n",
       "  'dominate',\n",
       "  'as',\n",
       "  'the',\n",
       "  'only',\n",
       "  'challenge',\n",
       "  'to',\n",
       "  'trump',\n",
       "  ',',\n",
       "  'and',\n",
       "  'without',\n",
       "  'a',\n",
       "  'real',\n",
       "  'vision',\n",
       "  'for',\n",
       "  'lifting',\n",
       "  'up',\n",
       "  'the',\n",
       "  'many',\n",
       "  'americans',\n",
       "  'enticed',\n",
       "  'by',\n",
       "  'trump',\n",
       "  '’s',\n",
       "  'campaign',\n",
       "  ',',\n",
       "  'the',\n",
       "  'nation',\n",
       "  'ended',\n",
       "  'up',\n",
       "  'with',\n",
       "  'an',\n",
       "  'end',\n",
       "  'result',\n",
       "  'similar',\n",
       "  'to',\n",
       "  'senator',\n",
       "  'john',\n",
       "  'kerry',\n",
       "  '’s',\n",
       "  'campaign',\n",
       "  ',',\n",
       "  'which',\n",
       "  'ran',\n",
       "  'primarily',\n",
       "  'on',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'he',\n",
       "  'was',\n",
       "  'not',\n",
       "  'president',\n",
       "  'george',\n",
       "  'w.',\n",
       "  'bush',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'it',\n",
       "  'did',\n",
       "  'not',\n",
       "  'help',\n",
       "  'the',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'that',\n",
       "  'she',\n",
       "  'had',\n",
       "  'a',\n",
       "  'reputation',\n",
       "  'for',\n",
       "  'supporting',\n",
       "  'regime',\n",
       "  'change',\n",
       "  'wars',\n",
       "  ',',\n",
       "  'which',\n",
       "  'have',\n",
       "  'greatly',\n",
       "  'destabilized',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  '.',\n",
       "  'her',\n",
       "  'fingerprints',\n",
       "  'were',\n",
       "  'all',\n",
       "  'over',\n",
       "  'the',\n",
       "  'libya',\n",
       "  'disaster',\n",
       "  '.',\n",
       "  'she',\n",
       "  'voted',\n",
       "  'for',\n",
       "  'the',\n",
       "  'iraq',\n",
       "  'war',\n",
       "  ',',\n",
       "  'which',\n",
       "  'created',\n",
       "  'the',\n",
       "  'conditions',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rise',\n",
       "  'of',\n",
       "  'the',\n",
       "  'islamic',\n",
       "  'state',\n",
       "  '.',\n",
       "  'and',\n",
       "  ',',\n",
       "  'although',\n",
       "  'it',\n",
       "  'is',\n",
       "  'questionable',\n",
       "  'whether',\n",
       "  'trump',\n",
       "  'really',\n",
       "  'ever',\n",
       "  'opposed',\n",
       "  'the',\n",
       "  'iraq',\n",
       "  'invasion',\n",
       "  ',',\n",
       "  'he',\n",
       "  'insisted',\n",
       "  'he',\n",
       "  'was',\n",
       "  'against',\n",
       "  'the',\n",
       "  'iraq',\n",
       "  'war',\n",
       "  'during',\n",
       "  'debates',\n",
       "  'to',\n",
       "  'undermine',\n",
       "  'clinton',\n",
       "  'and',\n",
       "  'fueled',\n",
       "  'the',\n",
       "  'perception',\n",
       "  'that',\n",
       "  'clinton',\n",
       "  'was',\n",
       "  'somehow',\n",
       "  'responsible',\n",
       "  'for',\n",
       "  'isis',\n",
       "  '.',\n",
       "  'trump',\n",
       "  'held',\n",
       "  'himself',\n",
       "  'out',\n",
       "  'as',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'would',\n",
       "  'not',\n",
       "  'plunge',\n",
       "  'the',\n",
       "  'country',\n",
       "  'into',\n",
       "  'reckless',\n",
       "  'military',\n",
       "  'engagements',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'clinton',\n",
       "  '’s',\n",
       "  'closing',\n",
       "  'argument',\n",
       "  'included',\n",
       "  'the',\n",
       "  'following',\n",
       "  ',',\n",
       "  '“',\n",
       "  'is',\n",
       "  'america',\n",
       "  'dark',\n",
       "  'and',\n",
       "  'divisive',\n",
       "  'or',\n",
       "  'helpful',\n",
       "  'and',\n",
       "  'inclusive',\n",
       "  '?',\n",
       "  'our',\n",
       "  'core',\n",
       "  'values',\n",
       "  'are',\n",
       "  'being',\n",
       "  'tested',\n",
       "  'in',\n",
       "  'this',\n",
       "  'election',\n",
       "  ',',\n",
       "  'but',\n",
       "  'everywhere',\n",
       "  'i',\n",
       "  'go',\n",
       "  ',',\n",
       "  'people',\n",
       "  'are',\n",
       "  'refusing',\n",
       "  'to',\n",
       "  'be',\n",
       "  'defined',\n",
       "  'by',\n",
       "  'fear',\n",
       "  'and',\n",
       "  'division',\n",
       "  '.',\n",
       "  'look',\n",
       "  ',',\n",
       "  'we',\n",
       "  'all',\n",
       "  'know',\n",
       "  'we',\n",
       "  '’ve',\n",
       "  'come',\n",
       "  'through',\n",
       "  'some',\n",
       "  'hard',\n",
       "  'economic',\n",
       "  'times',\n",
       "  ',',\n",
       "  'and',\n",
       "  'we',\n",
       "  '’ve',\n",
       "  'seen',\n",
       "  'some',\n",
       "  'pretty',\n",
       "  'big',\n",
       "  'changes',\n",
       "  '.',\n",
       "  'but',\n",
       "  'i',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'our',\n",
       "  'people',\n",
       "  '.',\n",
       "  'i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'country',\n",
       "  ',',\n",
       "  'and',\n",
       "  'i',\n",
       "  '’m',\n",
       "  'convinced',\n",
       "  'our',\n",
       "  'best',\n",
       "  'days',\n",
       "  'are',\n",
       "  'ahead',\n",
       "  'of',\n",
       "  'us',\n",
       "  'if',\n",
       "  'we',\n",
       "  'reach',\n",
       "  'for',\n",
       "  'them',\n",
       "  'together',\n",
       "  '.',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  'that',\n",
       "  'may',\n",
       "  'have',\n",
       "  'sounded',\n",
       "  'good',\n",
       "  'in',\n",
       "  'the',\n",
       "  'office',\n",
       "  'of',\n",
       "  'a',\n",
       "  'campaign',\n",
       "  '’s',\n",
       "  'headquarters',\n",
       "  ',',\n",
       "  'but',\n",
       "  'there',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'specific',\n",
       "  'in',\n",
       "  'this',\n",
       "  'buzzword',\n",
       "  '-',\n",
       "  'laden',\n",
       "  'pablum',\n",
       "  '.',\n",
       "  'multiculturalism',\n",
       "  'does',\n",
       "  'not',\n",
       "  'help',\n",
       "  'anyone',\n",
       "  'pay',\n",
       "  'their',\n",
       "  'mortgage',\n",
       "  'or',\n",
       "  'find',\n",
       "  'a',\n",
       "  'job',\n",
       "  '.',\n",
       "  'as',\n",
       "  'wrong',\n",
       "  'as',\n",
       "  'it',\n",
       "  'is',\n",
       "  'for',\n",
       "  'millions',\n",
       "  'of',\n",
       "  'white',\n",
       "  'americans',\n",
       "  'to',\n",
       "  'take',\n",
       "  'out',\n",
       "  'their',\n",
       "  'frustrations',\n",
       "  'on',\n",
       "  'people',\n",
       "  'of',\n",
       "  'color',\n",
       "  ',',\n",
       "  'the',\n",
       "  'system',\n",
       "  'failed',\n",
       "  'them',\n",
       "  'and',\n",
       "  'keeps',\n",
       "  'failing',\n",
       "  'them',\n",
       "  '.',\n",
       "  'additionally',\n",
       "  ',',\n",
       "  'establishment',\n",
       "  'politicians',\n",
       "  'like',\n",
       "  'clinton',\n",
       "  'wrote',\n",
       "  'off',\n",
       "  'many',\n",
       "  'of',\n",
       "  'these',\n",
       "  'people',\n",
       "  ',',\n",
       "  'believing',\n",
       "  'if',\n",
       "  'they',\n",
       "  'focused',\n",
       "  'on',\n",
       "  'emphasizing',\n",
       "  'diversity',\n",
       "  'they',\n",
       "  'would',\n",
       "  'overcome',\n",
       "  'the',\n",
       "  'painful',\n",
       "  'intertwined',\n",
       "  'realities',\n",
       "  'of',\n",
       "  'class',\n",
       "  'and',\n",
       "  'race',\n",
       "  'in',\n",
       "  'the',\n",
       "  'u.s.',\n",
       "  'they',\n",
       "  'were',\n",
       "  'wrong',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'let',\n",
       "  'us',\n",
       "  'go',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'belief',\n",
       "  'that',\n",
       "  'a',\n",
       "  'candidate',\n",
       "  'like',\n",
       "  'trump',\n",
       "  'would',\n",
       "  'be',\n",
       "  'perfect',\n",
       "  'for',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  '.',\n",
       "  'in',\n",
       "  'april',\n",
       "  '2015',\n",
       "  ',',\n",
       "  'a',\n",
       "  'strategy',\n",
       "  'memo',\n",
       "  'for',\n",
       "  'the',\n",
       "  'dnc',\n",
       "  'was',\n",
       "  'drafted',\n",
       "  'by',\n",
       "  'the',\n",
       "  'campaign',\n",
       "  'two',\n",
       "  'months',\n",
       "  'before',\n",
       "  'trump',\n",
       "  'announced',\n",
       "  'his',\n",
       "  'candidacy',\n",
       "  '.',\n",
       "  'the',\n",
       "  'goal',\n",
       "  'was',\n",
       "  'to',\n",
       "  '“',\n",
       "  'make',\n",
       "  'whomever',\n",
       "  'the',\n",
       "  'republicans',\n",
       "  'nominate',\n",
       "  'unpalatable',\n",
       "  'to',\n",
       "  'a',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'the',\n",
       "  'electorate',\n",
       "  '.',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  '“',\n",
       "  'force',\n",
       "  'all',\n",
       "  'republican',\n",
       "  'candidates',\n",
       "  'to',\n",
       "  'lock',\n",
       "  'themselves',\n",
       "  'into',\n",
       "  'extreme',\n",
       "  'conservative',\n",
       "  'positions',\n",
       "  'that',\n",
       "  'will',\n",
       "  'hurt',\n",
       "  'them',\n",
       "  'in',\n",
       "  'a',\n",
       "  'general',\n",
       "  'election',\n",
       "  ',',\n",
       "  '”',\n",
       "  'the',\n",
       "  'campaign',\n",
       "  'recommended',\n",
       "  '.',\n",
       "  '“',\n",
       "  'undermine',\n",
       "  'any',\n",
       "  'credibility',\n",
       "  '/',\n",
       "  'trust',\n",
       "  'republican',\n",
       "  'presidential',\n",
       "  'candidates',\n",
       "  'have',\n",
       "  'to',\n",
       "  'make',\n",
       "  'inroads',\n",
       "  'to',\n",
       "  'our',\n",
       "  'coalition',\n",
       "  'or',\n",
       "  'independents',\n",
       "  '.',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  'it',\n",
       "  'advocated',\n",
       "  'against',\n",
       "  'marginalizing',\n",
       "  '“',\n",
       "  'more',\n",
       "  'extreme',\n",
       "  'candidates',\n",
       "  '.',\n",
       "  '”',\n",
       "  'the',\n",
       "  'campaign',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'make',\n",
       "  '“',\n",
       "  'pied',\n",
       "  'piper',\n",
       "  'candidates',\n",
       "  ',',\n",
       "  '”',\n",
       "  'like',\n",
       "  'trump',\n",
       "  ',',\n",
       "  'senator',\n",
       "  'ted',\n",
       "  'cruz',\n",
       "  ',',\n",
       "  'and',\n",
       "  'ben',\n",
       "  'carson',\n",
       "  ',',\n",
       "  'into',\n",
       "  'representatives',\n",
       "  'of',\n",
       "  'the',\n",
       "  'republican',\n",
       "  'party',\n",
       "  '.',\n",
       "  '“',\n",
       "  'we',\n",
       "  'need',\n",
       "  'to',\n",
       "  'be',\n",
       "  'elevating',\n",
       "  'the',\n",
       "  'pied',\n",
       "  'piper',\n",
       "  'candidates',\n",
       "  'so',\n",
       "  'that',\n",
       "  'they',\n",
       "  'are',\n",
       "  'leaders',\n",
       "  'of',\n",
       "  'the',\n",
       "  'pack',\n",
       "  'and',\n",
       "  'tell',\n",
       "  'the',\n",
       "  'press',\n",
       "  'to',\n",
       "  '[',\n",
       "  'take',\n",
       "  ']',\n",
       "  'them',\n",
       "  'seriously',\n",
       "  '.',\n",
       "  '”',\n",
       "  '(',\n",
       "  'the',\n",
       "  'memo',\n",
       "  'was',\n",
       "  'attached',\n",
       "  'to',\n",
       "  'an',\n",
       "  'email',\n",
       "  'published',\n",
       "  'by',\n",
       "  'wikileaks',\n",
       "  '.',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'month',\n",
       "  ',',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'manager',\n",
       "  'robby',\n",
       "  'mook',\n",
       "  'pushed',\n",
       "  'for',\n",
       "  'a',\n",
       "  'primary',\n",
       "  'schedule',\n",
       "  ',',\n",
       "  'where',\n",
       "  'the',\n",
       "  'red',\n",
       "  'states',\n",
       "  'held',\n",
       "  'their',\n",
       "  'primaries',\n",
       "  'early',\n",
       "  '.',\n",
       "  'it',\n",
       "  'would',\n",
       "  'increase',\n",
       "  '“',\n",
       "  'the',\n",
       "  'likelihood',\n",
       "  'the',\n",
       "  'rs',\n",
       "  'nominate',\n",
       "  'someone',\n",
       "  'extreme',\n",
       "  '.',\n",
       "  '”',\n",
       "  '\\n',\n",
       "  'essentially',\n",
       "  ',',\n",
       "  'the',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'engaged',\n",
       "  'in',\n",
       "  'steps',\n",
       "  'that',\n",
       "  'would',\n",
       "  'help',\n",
       "  'ensure',\n",
       "  'trump',\n",
       "  'was',\n",
       "  'the',\n",
       "  'republican',\n",
       "  'presidential',\n",
       "  'nominee',\n",
       "  '.',\n",
       "  'their',\n",
       "  'acts',\n",
       "  'enabled',\n",
       "  'the',\n",
       "  'rise',\n",
       "  'of',\n",
       "  'trump',\n",
       "  ',',\n",
       "  'and',\n",
       "  'they',\n",
       "  'lost',\n",
       "  'to',\n",
       "  'the',\n",
       "  'opponent',\n",
       "  'they',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'face',\n",
       "  'because',\n",
       "  'they',\n",
       "  'made',\n",
       "  'the',\n",
       "  'same',\n",
       "  'mistakes',\n",
       "  'democrats',\n",
       "  'make',\n",
       "  'time',\n",
       "  'and',\n",
       "  'time',\n",
       "  'again',\n",
       "  '.',\n",
       "  'they',\n",
       "  'clung',\n",
       "  'to',\n",
       "  'failed',\n",
       "  'corporate',\n",
       "  'democratic',\n",
       "  'policies',\n",
       "  'that',\n",
       "  'have',\n",
       "  'devastated',\n",
       "  'this',\n",
       "  'country',\n",
       "  'for',\n",
       "  'the',\n",
       "  'past',\n",
       "  'two',\n",
       "  'decades',\n",
       "  ',',\n",
       "  'and',\n",
       "  'in',\n",
       "  'some',\n",
       "  'ways',\n",
       "  ',',\n",
       "  ...],\n",
       " 'label': 'FAKE',\n",
       " 'title': ['the',\n",
       "  'hubris',\n",
       "  'of',\n",
       "  'democratic',\n",
       "  'elites',\n",
       "  ',',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'gave',\n",
       "  'us',\n",
       "  'president',\n",
       "  'trump']}"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "vars(train_data[0])\n",
    "# print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "text.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors = \"glove.6B.100d\", \n",
    "unk_init = torch.Tensor.normal_)\n",
    "title.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
    "label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: x.title,\n",
    "    sort_within_batch = True,\n",
    "    # sort=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "    bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional=bidirectional, dropout = dropout )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        # unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "\n",
    "        return self.fc(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(text.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = text.vocab.stoi[text.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = text.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.2854, -0.1562, -0.3448,  ...,  1.5273, -0.0496, -0.3139],\n",
       "        [ 0.7317, -1.0654,  0.5485,  ...,  1.1625,  1.5158, -0.6169],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.4451,  0.4952, -0.2546,  ...,  0.1101,  0.0458,  0.1567],\n",
       "        [-0.2559, -0.2528,  0.0982,  ...,  0.3928, -0.0542,  0.0134],\n",
       "        [ 0.4679,  0.0109,  1.0769,  ...,  0.3389, -0.1455,  0.8972]])"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n        ...,\n        [-0.4451,  0.4952, -0.2546,  ...,  0.1101,  0.0458,  0.1567],\n        [-0.2559, -0.2528,  0.0982,  ...,  0.3928, -0.0542,  0.0134],\n        [ 0.4679,  0.0109,  1.0769,  ...,  0.3389, -0.1455,  0.8972]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = text.vocab.stoi[text.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, label):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == label).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text, text_lengths = batch.text\n",
    "        # print(\"-------batch.text is: -------\")\n",
    "        # print(batch.text)\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc =+ acc.item()\n",
    "    \n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            predictions = model(text, text.length).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elasped_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elasped_mins * 60))\n",
    "\n",
    "    return elasped_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------batch.text is: -------\n(tensor([[ 189,  742,  652,  ...,  293,   75,   36],\n        [1934,    8,  856,  ...,   18,  241, 2481],\n        [   3, 4613, 2409,  ...,  161,    3,  426],\n        ...,\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1],\n        [   1,    1,    1,  ...,    1,    1,    1]]), tensor([2917,  584,   15,  905, 1447,  746, 1105,  987, 2171, 2362,  717, 2975,\n        1836,  927,  704, 1919,  999, 2305,  861,  707,   89, 1061, 1172, 1220,\n        1357,  916, 1011,  993, 1485,  998,  112,  488, 3538,  624,  475,  509,\n         950, 1107,  649,  889, 3854,  742, 2812, 2433, 1209, 1077, 1216,  762,\n        1918, 2387, 2510,  812,  109,  449, 1967,  924,  747, 1413,  772,  149,\n        1434, 1347,  450,  716]))\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "`lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-64aa353695bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mvalidation_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-b25542eab85f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-------batch.text is: -------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-7227495da475>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text, text_lengths)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# pack sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpacked_embedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mpacked_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability."
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    validation_loss, validation_acc = evaluate(model, validation_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(mode.state_dict(), 'LSTM-Model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\Validation Loss: {validation:.3f} | Validation Acc: {validation_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LSTM-Model.pt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-831541398215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LSTM-Model.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LSTM-Model.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('LSTM-Model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
    "    indexed = [text.vocab.stoi[t] for t in tokenized]\n",
    "    "
   ]
  }
 ]
}