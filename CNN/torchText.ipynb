{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]downloading aclImdb_v1.tar.gz\n",
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:06<00:00, 12.6MB/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e6adfa7a8e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMDB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\datasets\\imdb.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, text_field, label_field, root, train, test, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         return super(IMDB, cls).splits(\n\u001b[0;32m     54\u001b[0m             \u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_field\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             train=train, validation=None, test=test, **kwargs)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         train_data = None if train is None else cls(\n\u001b[0;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(cls, root, check)\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.tgz'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.gz'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mext_inner\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.tar'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r:gz'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                         \u001b[0mdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmember\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmember\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmembers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                         \u001b[0mtar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmembers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.gz'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mgetmembers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# if we want to obtain a list of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1763\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# all members, we first have to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1764\u001b[0m                                 \u001b[1;31m# scan the whole archive.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2348\u001b[0m         \"\"\"\n\u001b[0;32m   2349\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2350\u001b[1;33m             \u001b[0mtarinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2351\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2352\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[1;31m# Advance the file pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2281\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2282\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mseek\u001b[1;34m(self, offset, whence)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREAD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mseek\u001b[1;34m(self, offset, whence)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# Read and discard data until we reach the desired position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.datasets.imdb.IMDB at 0x11d131bc688>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.imdb.IMDB object at 0x0000011D131BC688>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "When using a dict to specify fields with a csv file,skip_header must be False andthe file must have a header.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f5b50f0f9b64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                         \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                         \u001b[0mskip_header\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         train_data = None if train is None else cls(\n\u001b[1;32m---> 78\u001b[1;33m             os.path.join(path, train), **kwargs)\n\u001b[0m\u001b[0;32m     79\u001b[0m         val_data = None if validation is None else cls(\n\u001b[0;32m     80\u001b[0m             os.path.join(path, validation), **kwargs)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m                     raise ValueError('When using a dict to specify fields with a {} file,'\n\u001b[0;32m    262\u001b[0m                                      \u001b[1;34m'skip_header must be False and'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                                      'the file must have a header.'.format(format))\n\u001b[0m\u001b[0;32m    264\u001b[0m                 \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[0mfield_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When using a dict to specify fields with a csv file,skip_header must be False andthe file must have a header."
     ]
    }
   ],
   "source": [
    "fields = {'text': ('t', TEXT), 'label': ('l', LABEL)}\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = '../data',\n",
    "                                        train = 'news.csv',\n",
    "                                        test = 'news.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = 'news.json',\n",
    "                            test = 'news.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Stuff\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# spacy_en = spacy.load('en')\n",
    "# def tokenize(text)z:\n",
    "#     return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "title = Field(sequential=True, use_vocab=True, tokenize='spacy', lower=True)\n",
    "text = Field(sequential=True, use_vocab=True, tokenize='spacy', lower=True)\n",
    "label = Field(sequential=False, use_vocab=False, dtype = torch.float)\n",
    "\n",
    "fields = {'title': ('title', title), 'text': ('text', text), 'label': ('label', label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num of training:  6335\nNum of testing:  6335\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='',\n",
    "    train='news.csv',\n",
    "    test='news.csv',\n",
    "    format='csv',\n",
    "    fields=fields)\n",
    "print(\"Num of training: \", len(train_data))\n",
    "print(\"Num of testing: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num of training:  3563\nNum of validation:  1188\nNum of testing:  6335\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = train_data.split(split_ratio=0.75)\n",
    "print(\"Num of training: \", len(train_data))\n",
    "print(\"Num of validation: \", len(validation_data))\n",
    "print(\"Num of testing: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['title', 'text', 'label'])\ndict_values([['state', 'that', 'fired', 'pastor', 'demands', 'his', 'sermons', ',', 'notes'], ['state', 'that', 'fired', 'pastor', 'demands', 'his', 'sermons', ',', 'notes', \"'\", 'this', 'is', 'an', 'excessive', 'display', 'of', 'the', 'government', 'overreaching', 'its', 'authority', \"'\", 'published', ':', '3', 'mins', 'ago', 'about', '|', '|', 'archive', 'bob', 'unruh', 'joined', 'wnd', 'in', '2006', 'after', 'nearly', 'three', 'decades', 'with', 'the', 'associated', 'press', ',', 'as', 'well', 'as', 'several', 'upper', 'midwest', 'newspapers', ',', 'where', 'he', 'covered', 'everything', 'from', 'legislative', 'battles', 'and', 'sports', 'to', 'tornadoes', 'and', 'homicidal', 'survivalists', '.', 'he', 'is', 'also', 'a', 'photographer', 'whose', 'scenic', 'work', 'has', 'been', 'used', 'commercially', '.', 'print', 'dr.', 'eric', 'walsh', '(', 'photo', ':', 'first', 'liberty', ')', '\\n', 'the', 'state', 'of', 'georgia', 'is', 'demanding', 'copies', 'of', 'the', 'sermons', 'and', 'related', 'notes', 'of', 'a', 'lay', 'pastor', 'who', 'was', 'fired', 'by', 'the', 'department', 'of', 'public', 'health', 'after', 'it', 'investigated', 'what', 'he', 'said', 'in', 'his', 'church', '.', '\\n', 'but', 'dr.', 'eric', 'walsh', 'is', 'resisting', ',', 'issuing', 'a', 'statement', 'via', 'his', 'legal', 'team', 'that', 'he', 'will', 'not', 'comply', 'with', 'the', 'demand', 'from', 'state', 'lawyers', '.', '\\n', 'the', 'state', '’s', 'demand', 'is', 'in', 'response', 'to', 'a', 'lawsuit', 'filed', 'by', 'walsh', 'against', 'the', 'department', 'of', 'health', 'charging', 'discrimination', 'based', 'on', 'his', 'religion', 'and', 'other', 'civil', 'rights', 'violations', '.', '\\n', 'he', '’s', 'getting', 'support', 'from', 'a', 'pastor', 'who', 'successfully', 'fought', 'off', 'a', 'demand', 'by', 'houston', 'officials', 'for', 'copies', 'of', 'his', 'sermons', '.', '\\n', 'walsh', '’s', 'ordeal', 'began', 'in', 'may', '2014', 'when', 'he', 'accepted', 'an', 'offer', 'as', 'to', 'become', 'district', 'health', 'director', 'with', 'the', 'state', 'agency', '.', 'only', 'a', 'week', 'later', ',', 'a', 'state', 'official', 'asked', 'him', 'to', 'provide', 'copies', 'of', 'sermons', 'he', 'had', 'preached', 'as', 'a', 'lay', 'minister', 'with', 'a', 'seventh', '-', 'day', 'adventist', 'church', '.', '\\n', 'lee', 'rudd', ',', 'the', 'agency', '’s', 'human', 'resources', 'director', ',', 'then', 'assigned', 'staff', 'members', 'to', 'listen', 'to', 'the', 'youtube', 'recordings', 'immediately', '.', 'two', 'days', 'later', ',', 'walsh', 'was', 'fired', '.', '\\n', 'at', 'that', 'point', ',', 'lawyers', 'with', 'first', 'liberty', 'institute', 'joined', 'forces', 'with', 'the', 'atlanta', 'legal', 'team', 'of', 'parks', ',', 'chesin', '&', 'walbert', 'to', 'file', 'a', 'federal', 'lawsuit', 'against', 'the', 'state', 'agency', '.', '\\n', '“', 'police', 'state', 'usa', ':', 'how', 'orwell', '’s', 'nightmare', 'is', 'becoming', 'our', 'reality', '”', 'chronicles', 'how', 'america', 'has', 'arrived', 'at', 'the', 'point', 'of', 'being', 'a', 'de', 'facto', 'police', 'state', ',', 'and', 'what', 'led', 'to', 'an', 'out', '-', 'of', '-', 'control', 'government', 'that', 'increasingly', 'ignores', 'the', 'constitution', '.', 'order', 'today', '!', '\\n', 'now', ',', 'in', 'response', 'to', 'walsh', '’s', 'lawsuit', ',', 'the', 'state', 'delivered', 'a', '“', 'request', 'for', 'production', 'of', 'documents', '”', 'that', 'demands', ',', 'among', 'a', 'flood', 'of', 'other', 'paperwork', ',', '“', 'copies', 'of', 'his', 'sermon', 'notes', 'and', 'transcripts', '.', '”', '\\n', '“', 'this', 'is', 'an', 'excessive', 'display', 'of', 'the', 'government', 'overreaching', 'its', 'authority', 'and', 'violating', 'the', 'sanctity', 'of', 'the', 'church', ',', '”', 'said', 'jeremy', 'dys', ',', 'senior', 'counsel', 'for', 'first', 'liberty', '.', '\\n', '“', 'no', 'government', 'has', 'the', 'right', 'to', 'require', 'a', 'pastor', 'to', 'turn', 'over', 'his', 'sermons', ',', '”', 'said', 'walsh', 'in', 'a', 'statement', 'released', 'by', 'his', 'lawyers', '.', '“', 'i', 'can', 'not', 'and', 'will', 'not', 'give', 'up', 'my', 'sermons', 'unless', 'i', 'am', 'forced', 'to', 'do', 'so', '.', '”', '\\n', 'officials', 'with', 'the', 'georgia', 'department', 'of', 'health', 'declined', 'to', 'respond', 'to', 'a', 'wnd', 'request', 'for', 'comment', ',', 'instead', 'referring', 'a', 'reporter', 'to', 'the', 'state', 'attorney', 'general', ',', 'who', 'did', 'not', 'respond', 'to', 'a', 'request', 'for', 'comment', '.', '\\n', 'walsh', '’s', 'lawyers', 'scheduled', 'a', 'news', 'conference', 'as', 'a', 'display', 'of', 'support', '.', '\\n', 'on', 'the', 'guest', 'list', 'was', 'pastor', 'dave', 'welch', 'of', 'houston', ',', 'one', 'of', 'five', 'pastors', 'whose', 'sermons', 'were', 'demanded', 'by', 'a', 'lesbian', 'mayor', 'during', 'her', 'campaign', 'to', 'establish', 'protections', 'for', 'her', 'sexual', 'preferences', 'in', 'city', 'code', '.', '\\n', 'wnd', 'broke', 'the', 'story', 'when', 'the', 'city', 'launched', 'its', 'action', 'against', 'the', 'pastors', 'and', 'also', 'reported', 'when', 'rush', 'limbaugh', 'described', 'parker', '’s', 'actions', 'as', 'possibly', '“', 'one', 'of', 'the', 'most', 'vile', ',', 'filthy', ',', 'blatant', 'violations', 'of', 'the', 'constitution', 'that', 'i', 'have', 'seen', '.', '”', '\\n', 'the', 'mayor', 'at', 'the', 'time', ',', 'annise', 'parker', ',', 'withdrew', 'the', 'demands', 'amid', 'a', 'flood', 'of', 'protest', '.', '\\n', 'in', 'a', 'prepared', 'statement', 'wednesday', 'on', 'walsh', '’s', 'case', ',', 'welch', 'said', ',', '“', 'i', 'ca', 'n’t', 'believe', 'i', '’m', 'saying', 'this', ',', 'but', 'georgia', '’s', 'demand', 'is', 'even', 'worse', 'than', 'when', 'the', 'mayor', 'of', 'houston', 'demanded', '17', 'different', 'categories', 'of', 'materials', ',', 'including', 'sermons', ',', 'from', '…', 'us', '.', '”', '\\n', 'welch', ',', 'the', 'executive', 'director', 'of', 'the', 'texas', 'pastor', 'council', ',', 'said', 'what', 'is', 'happening', 'to', 'walsh', 'is', '“', 'worse', 'than', 'what', 'happened', 'in', 'houston', 'for', 'multiple', 'reasons', '.', '”', '\\n', '“', 'first', ',', 'this', 'is', 'state', 'government', 'coming', 'after', 'a', 'pastor', ',', 'not', 'just', 'a', 'rogue', 'mayor', 'in', 'one', 'city', ',', '”', 'he', 'said', '.', '“', 'also', ',', 'the', 'state', 'is', 'demanding', 'much', 'more', 'material', ':', 'sermons', ',', 'sermon', 'notes', ',', 'all', 'documents', 'without', 'even', 'topical', 'or', 'time', 'limits', '.', 'it', 'could', 'even', 'include', 'margin', 'notes', 'in', 'this', 'pastor', '’s', 'preaching', 'bible', '.', 'it', '’s', 'almost', 'as', 'if', 'they', 'are', 'ransacking', 'the', 'pastor', '’s', 'study', '.', 'this', 'sweeping', 'demand', 'is', 'ominous', 'and', 'a', 'threat', 'to', 'every', 'pastor', ',', 'every', 'church', ',', 'every', 'denomination', ',', 'and', 'every', 'citizen', 'of', 'faith', 'in', 'america', '.', '”', '\\n', 'leaders', 'of', 'concerned', 'women', 'for', 'america', 'legislative', 'action', 'committee', ',', 'part', 'of', 'the', 'nation', '’s', 'largest', 'public', 'policy', 'women', '’s', 'group', 'with', '500,000', 'members', ',', 'also', 'came', 'to', 'walsh', '’s', 'defense', '.', '\\n', 'penny', 'nance', ',', 'ceo', ',', 'said', ':', '“', 'the', 'words', 'of', 'reverend', 'dr.', 'martin', 'luther', 'king', ',', 'jr.', ',', 'that', ',', '‘', 'injustice', 'anywhere', 'is', 'a', 'threat', 'to', 'justice', 'everywhere', ',', '’', 'still', 'reverberate', 'today', '–', 'especially', 'as', 'we', 'witness', 'the', '‘', 'gestapo', '-', 'like', '’', 'tactics', 'of', 'his', 'native', 'state', '.', 'the', 'state', 'of', 'georgia', '’s', 'blatant', 'attack', 'on', 'religious', 'freedom', ',', 'as', 'they', 'discriminate', 'against', 'another', 'pastor', ',', 'dr.', 'eric', 'walsh', ',', 'is', 'indeed', 'a', 'threat', 'to', 'every', 'american', ',', 'whatever', 'our', 'religious', 'beliefs', '.', '\\n', '“', 'can', 'there', 'be', 'a', 'clearer', 'violation', 'of', 'our', 'first', 'amendment', 'right', 'to', 'religious', 'freedom', 'than', 'for', 'the', 'state', 'to', 'monitor', ',', 'examine', ',', 'and', 'retaliate', 'against', 'a', 'person', 'because', 'of', 'the', 'sermons', 'they', 'share', '?', '”', '\\n', 'wnd', 'reported', 'earlier', 'on', 'the', 'case', 'brought', 'against', 'the', 'state', 'after', 'its', 'officials', 'reviewed', 'walsh', '’s', 'sermons', 'and', 'then', 'fired', 'him', '.', '\\n', '“', 'no', 'one', 'in', 'this', 'country', 'should', 'be', 'fired', 'from', 'their', 'job', 'for', 'something', 'that', 'was', 'said', 'in', 'a', 'church', 'or', 'from', 'a', 'pulpit', 'during', 'a', 'sermon', ',', '”', 'dys', 'told', 'fox', 'news', 'when', 'the', 'case', 'was', 'filed', '.', '“', 'he', 'was', 'fired', 'for', 'something', 'he', 'said', 'in', 'a', 'sermon', '.', 'if', 'the', 'government', 'is', 'allowed', 'to', 'fire', 'someone', 'over', 'what', 'he', 'said', 'in', 'his', 'sermons', ',', 'they', 'can', 'come', 'after', 'any', 'of', 'us', 'for', 'our', 'beliefs', 'on', 'anything', '.', '”', 'the', 'original', 'state', 'investigation', 'of', 'walsh', '’s', 'sermons', 'apparently', 'was', 'sparked', 'by', '“', 'one', 'complaint', '”', 'from', 'an', 'official', 'with', 'a', 'county', 'democratic', 'party', 'and', '“', 'gay', 'activist', '.', '”', '\\n', 'state', 'officials', 'also', 'joked', 'about', 'informing', 'walsh', 'of', 'his', 'firing', '.', '\\n', 'the', 'telephone', 'call', 'was', 'between', 'dr.', 'patrick', 'o’neal', ',', 'an', 'agency', 'official', ',', 'and', 'kate', 'pfirman', ',', 'an', 'agency', 'financial', 'officer', '.', 'the', 'call', 'was', 'captured', 'on', 'an', 'answering', 'machine', ',', 'which', 'also', 'caught', 'their', 'conversation', 'after', 'they', 'thought', 'they', 'had', 'hung', 'up', '.', '\\n', 'pfirman', 'said', ':', '“', 'and', 'i', '’m', 'gon', 'na', 'be', 'very', '–', 'i', '’m', 'gon', 'na', 'try', 'to', 'come', 'off', 'as', 'very', 'cold', ',', 'because', 'i', 'do', 'n’t', 'want', 'to', 'say', 'very', 'much', '.', 'if', 'i', 'try', 'to', 'make', 'it', 'warm', '–', 'i', '’ve', 'thought', 'that', 'through', ',', 'it', '’s', 'gon', 'na', 'just', 'not', '–', 'there', '’s', 'no', 'warm', 'way', 'to', 'say', 'it', 'anyway', '.', '”', '\\n', 'then', 'there', 'was', 'laughter', 'from', 'both', 'parties', '.', '\\n', 'o’neal', 'then', 'said', 'to', 'inform', 'walsh', ',', '“', 'you', '’re', 'out', ',', '”', 'and', 'there', 'was', 'another', 'round', 'of', 'laughter', '.', '\\n', '“', 'it', '’s', 'very', 'funny', ',', '”', 'pfirman', 'said', '.', '\\n', 'the', 'voicemail', ':', '\\n', 'in', 'the', 'houston', 'dispute', ',', 'voters', 'ultimately', 'soundly', 'rejected', 'parker', '’s', 'ordinance', 'giving', '“', 'gays', '”', 'and', 'transgendered', 'people', 'special', 'rights', '.'], 'FAKE'])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "text.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "title.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "1f307c3022c27ca4762c821a3aebb95f65fea64f739e5b86e4575fdd25133081"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}